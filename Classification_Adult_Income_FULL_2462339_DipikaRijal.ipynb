{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZsZjNDnRdcHM"
      },
      "source": [
        "# Classification Project: Adult Census Income (UCI)\n",
        "\n",
        "## 1. Problem Statement\n",
        "We build a supervised **binary classification** model to predict whether an individual's annual income is **> $50K** (`income = 1`) or **<= $50K** (`income = 0`) using demographic and employment-related attributes.\n",
        "\n",
        "## 2. UNSDG Alignment\n",
        "This project aligns with **UN Sustainable Development Goal 8 (Decent Work and Economic Growth)** because it studies relationships between education, occupation, working hours, and income—key factors that inform policies for productive employment and improved living standards.\n",
        "\n",
        "## 3. Dataset (required details)\n",
        "**Dataset:** *Adult / Census Income* (UCI Machine Learning Repository)\n",
        "\n",
        "- **Created / extracted:** Extracted from the **1994 U.S. Census database** by **Barry Becker**.\n",
        "- **Donated / published on UCI:** **April 30, 1996**.\n",
        "- **Access / source:** Downloaded as a CSV file from the UCI Machine Learning Repository.\n",
        "- **Instances / features:** 48,842 rows and 14 input features (target not included in the 14).\n",
        "- **Known issues:** Missing values exist for some categorical fields (often encoded as `?`).\n",
        "\n",
        "**Primary reference:** UCI dataset page (Adult/Census Income).\n",
        "\n",
        "## 4. Questions this dataset can answer (examples)\n",
        "1. Which attributes (e.g., education level, occupation, working hours) are most associated with earning **> $50K**?\n",
        "2. Are higher-income individuals older on average, or do they work more hours per week?\n",
        "3. How does income vary by education and marital status?\n",
        "\n",
        "---\n",
        "\n",
        "> **Note:** The notebook uses a clean sklearn pipeline approach (imputation → encoding/scaling → model), so there is **no data leakage** between train and test.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-KshMDWMht37"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV, StratifiedKFold, cross_val_score\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, precision_score, recall_score, f1_score,\n",
        "    classification_report, confusion_matrix\n",
        ")\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "np.random.seed(RANDOM_STATE)\n",
        "\n",
        "FAST_MODE = True        # True = quicker experiments, False = full dataset run\n",
        "SAMPLE_FRAC = 0.25\n",
        "\n",
        "sns.set_theme(style=\"whitegrid\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS6liM1VdcHR"
      },
      "source": [
        "## 3. Dataset (Adult / Census Income)\n",
        "\n",
        "**Source:** UCI Machine Learning Repository (Adult / Census Income). The UCI page states the dataset was **donated on 30 April 1996**, and the extraction was done by **Barry Becker** from the **1994 US Census database**. (Commonly referenced contributors: Ron Kohavi and Barry Becker.)\n",
        "\n",
        "**How the dataset was accessed in this notebook:** A local CSV file `adult.csv` (same schema as UCI Adult). If you do not have the file, download it from the UCI repository and rename to `adult.csv`.\n",
        "\n",
        "### 3.1 Attributes (Features)\n",
        "Each row represents one individual. The target label is `income`.\n",
        "\n",
        "| Attribute | Type | Brief meaning |\n",
        "|---|---:|---|\n",
        "| age | numeric | Age in years |\n",
        "| workclass | categorical | Employment type (e.g., Private, State-gov, Self-emp) |\n",
        "| fnlwgt | numeric | Census sampling weight |\n",
        "| education | categorical | Highest education level |\n",
        "| education-num | numeric | Education level encoded as an integer |\n",
        "| marital-status | categorical | Marital status |\n",
        "| occupation | categorical | Occupation category |\n",
        "| relationship | categorical | Relationship role in family |\n",
        "| race | categorical | Race category |\n",
        "| sex | categorical | Sex (Female/Male) |\n",
        "| capital-gain | numeric | Capital gains |\n",
        "| capital-loss | numeric | Capital losses |\n",
        "| hours-per-week | numeric | Working hours per week |\n",
        "| native-country | categorical | Country of origin |\n",
        "| income (target) | categorical | `<=50K` or `>50K` |\n",
        "\n",
        "### 3.2 Example questions the dataset can answer\n",
        "1. Which attributes (education, occupation, hours/week, capital-gain, etc.) are most associated with earning **> $50K**?\n",
        "2. How does **education level** affect the probability of earning **> $50K**?\n",
        "3. Can we build a model that reliably predicts higher income while controlling for class imbalance?\n",
        "\n",
        "### 3.3 Initial quality considerations\n",
        "- The UCI Adult dataset includes **missing values** in some categorical columns (often encoded as `?`).\n",
        "- The target is typically **imbalanced** (more `<=50K` than `>50K`), so we report **precision, recall, F1**, and also ROC-AUC / PR-AUC where useful.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMPLQuvJh1pl"
      },
      "source": [
        "**Interpretation **\n",
        "\n",
        "This cell imports all libraries for data handling, visualization, machine learning, and evaluation. A fixed random seed is used so results are reproducible (same split and similar outcomes each run). FAST_MODE allows faster testing by sampling the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Y8873Ifiht1j"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"adult.csv\", na_values=[\"?\"])\n",
        "print(\"Shape:\", df.shape)\n",
        "display(df.head())\n",
        "\n",
        "df = df.copy()\n",
        "\n",
        "# Clean whitespace for text columns\n",
        "for c in df.select_dtypes(include=\"object\").columns:\n",
        "    df[c] = df[c].astype(str).str.strip()\n",
        "    df.loc[df[c].str.lower() == \"nan\", c] = np.nan\n",
        "\n",
        "# Ensure pandas NA becomes numpy nan (sklearn-friendly)\n",
        "df = df.replace({pd.NA: np.nan})\n",
        "\n",
        "TARGET = \"income\"\n",
        "y_full = (df[TARGET] == \">50K\").astype(int)\n",
        "X_full = df.drop(columns=TARGET)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nJ74AgdqiJ2b"
      },
      "source": [
        "Interpretation:\n",
        "Here we load the dataset and handle missing values (the dataset uses ? as missing). We also remove extra spaces from categorical values (like \" Private\"), which improves encoding quality. The target (income) is converted into binary classes: 1 for >50K and 0 for <=50K."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Initial data quality check on raw (uncleaned) data\n",
        "print(\"Missing values before cleaning:\")\n",
        "display(df.isna().sum()[df.isna().sum() > 0])\n",
        "\n",
        "print(\"Duplicate rows before cleaning:\", df.duplicated().sum())\n"
      ],
      "metadata": {
        "id": "RzEFUtLoemgm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yV5KqI6HdcHS"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Data dictionary (feature descriptions)\n",
        "data_dict = pd.DataFrame([\n",
        "    (\"age\", \"numeric (int)\", \"Age in years.\"),\n",
        "    (\"workclass\", \"categorical\", \"Employer type (e.g., Private, Self-emp, Government, etc.).\"),\n",
        "    (\"fnlwgt\", \"numeric (int)\", \"Final sampling weight used by the Census Bureau (proxy for representation in population).\"),\n",
        "    (\"education\", \"categorical\", \"Highest education level (string label).\"),\n",
        "    (\"education-num\", \"numeric (int)\", \"Education level as an ordered numeric value.\"),\n",
        "    (\"marital-status\", \"categorical\", \"Marital status category.\"),\n",
        "    (\"occupation\", \"categorical\", \"Occupation category (e.g., Exec-managerial, Sales, etc.).\"),\n",
        "    (\"relationship\", \"categorical\", \"Relationship / household role (e.g., Husband, Wife, Not-in-family).\"),\n",
        "    (\"race\", \"categorical\", \"Race category.\"),\n",
        "    (\"sex\", \"binary categorical\", \"Sex (Female/Male).\"),\n",
        "    (\"capital-gain\", \"numeric (int)\", \"Capital gains recorded for the year.\"),\n",
        "    (\"capital-loss\", \"numeric (int)\", \"Capital losses recorded for the year.\"),\n",
        "    (\"hours-per-week\", \"numeric (int)\", \"Hours worked per week.\"),\n",
        "    (\"native-country\", \"categorical\", \"Country of origin.\"),\n",
        "    (\"income (target)\", \"binary\", \"Target: >50K or <=50K annual income.\")\n",
        "], columns=[\"Attribute\", \"Type\", \"Description\"])\n",
        "\n",
        "display(data_dict)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0VGkV20AhtzD"
      },
      "outputs": [],
      "source": [
        "# Stratified sampling and quick data quality checks\n",
        "\n",
        "# Use a smaller, representative subset of data when FAST_MODE is enabled.\n",
        "# This speeds up experimentation while keeping the class ratio similar.\n",
        "if FAST_MODE:\n",
        "    tmp = X_full.copy()\n",
        "\n",
        "    # Add target temporarily so we can sample within each class\n",
        "    tmp[\"_y\"] = y_full\n",
        "\n",
        "    # Sample within each class to preserve the original class distribution\n",
        "    tmp = tmp.groupby(\"_y\", observed=True).sample(\n",
        "        frac=SAMPLE_FRAC,\n",
        "        random_state=RANDOM_STATE\n",
        "    )\n",
        "\n",
        "    # Separate features and target again after sampling\n",
        "    y = tmp[\"_y\"]\n",
        "    X = tmp.drop(columns=\"_y\")\n",
        "\n",
        "    print(f\"FAST_MODE=True → using {len(X):,} rows\")\n",
        "else:\n",
        "    # Full dataset for final training and reporting\n",
        "    X, y = X_full, y_full\n",
        "    print(f\"FAST_MODE=False → using full dataset: {len(X):,} rows\")\n",
        "\n",
        "# Human-readable labels for the target classes\n",
        "labels = {0: \"<=50K\", 1: \">50K\"}\n",
        "\n",
        "# Show class distribution (helps spot imbalance before modeling)\n",
        "display(pd.DataFrame({\n",
        "    \"Class\": [labels[i] for i in y.value_counts().sort_index().index],\n",
        "    \"Count\": y.value_counts().sort_index().values,\n",
        "    \"Percent\": (y.value_counts(normalize=True).sort_index() * 100).round(2).values\n",
        "}))\n",
        "\n",
        "# Check missing values in X (should be 0 if you already dropped missing rows)\n",
        "missing_pct = (X.isna().mean() * 100).sort_values(ascending=False)\n",
        "display(missing_pct[missing_pct > 0].round(2).to_frame(\"Missing %\"))\n",
        "\n",
        "# Check duplicates in the original dataframe (before/after cleaning depends on where df is defined)\n",
        "print(\"Duplicate rows in raw df:\", df.duplicated().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ZlQZrvFiWVE"
      },
      "source": [
        "Interpretation:\n",
        "This cell checks dataset quality. The target distribution shows class imbalance (more <=50K than >50K), so F1-score and recall become important, not just accuracy. We also measure missing values and duplicates to understand cleaning needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lf9lDVt3htwc"
      },
      "outputs": [],
      "source": [
        "\n",
        "# 4. Exploratory Data Analysis (EDA)\n",
        "\n",
        "# Sanity check after cleaning\n",
        "print(\"Total missing values in X:\", X.isna().sum().sum())\n",
        "\n",
        "# Quick structural summary\n",
        "display(pd.DataFrame({\n",
        "    \"dtype\": X.dtypes,\n",
        "    \"n_missing\": X.isna().sum(),\n",
        "    \"missing_%\": (X.isna().mean()*100).round(2),\n",
        "    \"n_unique\": X.nunique()\n",
        "}).sort_values(\"missing_%\", ascending=False))\n",
        "\n",
        "# Summary statistics (numeric)\n",
        "display(X.describe(include=[np.number]).T)\n",
        "\n",
        "# 4.1 Target distribution (class imbalance check)\n",
        "plt.figure(figsize=(7,4))\n",
        "sns.countplot(x=y.map(labels))\n",
        "plt.title(\"Income Class Distribution\")\n",
        "plt.xlabel(\"Income\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 4.2 Missingness overview\n",
        "miss_nonzero = missing_pct[missing_pct > 0].sort_values()\n",
        "if len(miss_nonzero) > 0:\n",
        "    plt.figure(figsize=(8,4))\n",
        "    miss_nonzero.plot(kind=\"barh\")\n",
        "    plt.title(\"Missing Values by Column (%)\")\n",
        "    plt.xlabel(\"Missing %\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Identify numeric vs categorical columns\n",
        "num_cols = X.select_dtypes(include=[\"int64\", \"float64\"]).columns.tolist()\n",
        "cat_cols = X.select_dtypes(exclude=[\"int64\", \"float64\"]).columns.tolist()\n",
        "print(\"Numeric columns:\", num_cols)\n",
        "print(\"Categorical columns:\", cat_cols)\n",
        "\n",
        "# 4.3 Age distribution + relationship with income\n",
        "if \"age\" in X.columns:\n",
        "    plt.figure(figsize=(7,4))\n",
        "    sns.histplot(data=X, x=\"age\", bins=30, kde=True)\n",
        "    plt.title(\"Distribution of Age\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    tmp_plot = X[[\"age\"]].copy()\n",
        "    tmp_plot[\"income\"] = y.map(labels)\n",
        "    plt.figure(figsize=(7,4))\n",
        "    sns.boxplot(data=tmp_plot, x=\"income\", y=\"age\")\n",
        "    plt.title(\"Age by Income Group\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 4.4 Hours-per-week and income\n",
        "if \"hours-per-week\" in X.columns:\n",
        "    plt.figure(figsize=(7,4))\n",
        "    sns.histplot(data=X, x=\"hours-per-week\", bins=30, kde=True)\n",
        "    plt.title(\"Distribution of Hours Worked per Week\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    tmp_plot = X[[\"hours-per-week\"]].copy()\n",
        "    tmp_plot[\"income\"] = y.map(labels)\n",
        "    plt.figure(figsize=(7,4))\n",
        "    sns.boxplot(data=tmp_plot, x=\"income\", y=\"hours-per-week\")\n",
        "    plt.title(\"Hours-per-week by Income Group\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 4.5 Education vs income (income rate)\n",
        "if \"education\" in X.columns:\n",
        "    edu_rate = pd.DataFrame({\"education\": X[\"education\"], \"y\": y}).groupby(\"education\")[\"y\"].mean()\n",
        "    edu_count = X[\"education\"].value_counts()\n",
        "    edu_plot = pd.DataFrame({\"income_rate\": edu_rate, \"count\": edu_count}).sort_values(\"count\", ascending=False).head(12)\n",
        "    plt.figure(figsize=(10,4))\n",
        "    sns.barplot(x=edu_plot.index, y=edu_plot[\"income_rate\"])\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.title(\"Income Rate (>50K) by Education (Top 12 by frequency)\")\n",
        "    plt.ylabel(\"P(income > 50K)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 4.6 Occupation vs income (top occupations)\n",
        "if \"occupation\" in X.columns:\n",
        "    occ_count = X[\"occupation\"].value_counts().dropna()\n",
        "    top_occ = occ_count.head(12).index\n",
        "    occ_rate = pd.DataFrame({\"occupation\": X[\"occupation\"], \"y\": y}).dropna().groupby(\"occupation\")[\"y\"].mean()\n",
        "    occ_plot = occ_rate.loc[top_occ].sort_values(ascending=False)\n",
        "    plt.figure(figsize=(10,4))\n",
        "    sns.barplot(x=occ_plot.index, y=occ_plot.values)\n",
        "    plt.xticks(rotation=45, ha=\"right\")\n",
        "    plt.title(\"Income Rate (>50K) by Occupation (Top 12 by frequency)\")\n",
        "    plt.ylabel(\"P(income > 50K)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 4.7 Capital gain is extremely right-skewed (many zeros). Use log1p for a clearer view.\n",
        "if \"capital-gain\" in X.columns:\n",
        "    cg = X[\"capital-gain\"].fillna(0)\n",
        "    plt.figure(figsize=(7,4))\n",
        "    sns.histplot(np.log1p(cg), bins=40, kde=True)\n",
        "    plt.title(\"log(1 + capital-gain) Distribution\")\n",
        "    plt.xlabel(\"log1p(capital-gain)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# 4.8 Correlation heatmap (numeric features only)\n",
        "if len(num_cols) >= 2:\n",
        "    plt.figure(figsize=(7,5))\n",
        "    sns.heatmap(X[num_cols].corr(numeric_only=True), annot=True, fmt=\".2f\", linewidths=0.5)\n",
        "    plt.title(\"Correlation Heatmap (Numeric Features)\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WdVo8enCdcHT"
      },
      "source": [
        "### Key EDA Insights (what we learned)\n",
        "\n",
        "- **Class imbalance:** The dataset contains more `<=50K` than `>50K`, so we prioritize **F1-score**, **recall**, and **ROC-AUC/PR-AUC** instead of only accuracy.\n",
        "- **Missing values:** Missingness appears mainly in work-related categorical fields (e.g., *workclass*, *occupation*). We handle this with **imputation + one-hot encoding** in a preprocessing pipeline to avoid data leakage.\n",
        "- **Age & hours worked:** Higher-income individuals generally show higher median *age* and higher *hours-per-week*.\n",
        "- **Education & occupation:** Higher education levels and professional/managerial occupations have substantially higher `P(income > 50K)`.\n",
        "- **Capital-gain:** Highly right-skewed (many zeros). The log1p view confirms a small group with very high gains.\n",
        "\n",
        "These patterns justify (1) robust preprocessing, (2) models that can capture nonlinear interactions (e.g., Random Forest), and (3) evaluation beyond accuracy.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KhvnuE5Vhtt7"
      },
      "outputs": [],
      "source": [
        "# Split the dataset into training and testing sets\n",
        "# Stratification ensures both income classes are represented proportionally\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=RANDOM_STATE\n",
        ")\n",
        "\n",
        "# Print shapes to verify the split\n",
        "print(\"Train/Test:\", X_train.shape, X_test.shape)\n",
        "\n",
        "# Preprocessing for numerical features\n",
        "# Median imputation is robust to outliers, followed by standard scaling\n",
        "numeric_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"median\")),\n",
        "    (\"scaler\", StandardScaler())\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical features\n",
        "# Missing values are filled with the most frequent category,\n",
        "# then features are one-hot encoded\n",
        "categorical_pipe = Pipeline([\n",
        "    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n",
        "    (\"onehot\", OneHotEncoder(handle_unknown=\"ignore\"))\n",
        "])\n",
        "\n",
        "# Combine numerical and categorical preprocessing steps\n",
        "# ColumnTransformer applies the correct pipeline to each feature type\n",
        "preprocess = ColumnTransformer([\n",
        "    (\"num\", numeric_pipe, num_cols),\n",
        "    (\"cat\", categorical_pipe, cat_cols)\n",
        "])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5ezatDWilrc"
      },
      "source": [
        "Interpretation:\n",
        "We split into train and test sets using stratification so the income class ratio stays similar in both splits. Then we build a preprocessing pipeline: numeric features are imputed with median and scaled, while categorical features are imputed with the most frequent value and one-hot encoded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oirgSPoFdcHU"
      },
      "source": [
        "## 5.1 Neural Network Model (MLPClassifier)\n",
        "\n",
        "We implement a **Multi-Layer Perceptron (MLP)** as the neural network classifier.\n",
        "\n",
        "**Architecture (chosen for tabular data):**\n",
        "- Input: preprocessed numeric + one-hot categorical features\n",
        "- Hidden layers: **(64, 32)** neurons\n",
        "- Activation: **ReLU** (non-linear, works well for deep networks)\n",
        "- Output layer: **sigmoid**-like probability via sklearn's internal logistic output for binary classification\n",
        "\n",
        "**Training setup:**\n",
        "- Loss: **log-loss (cross-entropy)** (default for classification in `MLPClassifier`)\n",
        "- Optimizer: **Adam** (default, good general-purpose adaptive optimizer)\n",
        "- Regularization: L2 controlled by `alpha`\n",
        "- Early stopping: enabled to reduce overfitting\n",
        "\n",
        "We evaluate using metrics suitable for imbalanced classes: **F1**, **recall**, and (when available) **ROC-AUC/PR-AUC**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uooSb7YyhtrL"
      },
      "outputs": [],
      "source": [
        "# Baseline Models (2 Classical + 1 Neural Network) and Evaluation\n",
        "\n",
        "# Extra metrics and curve utilities (useful when classes are not perfectly balanced)\n",
        "from sklearn.metrics import roc_auc_score, average_precision_score, RocCurveDisplay, PrecisionRecallDisplay\n",
        "\n",
        "def evaluate(model, name, show_curves=False):\n",
        "    \"\"\"Train the model and evaluate on the test set using multiple classification metrics.\"\"\"\n",
        "\n",
        "    # Fit the model on training data\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict class labels on the test set\n",
        "    pred = model.predict(X_test)\n",
        "\n",
        "    # Try to get predicted probabilities for ROC-AUC and PR-AUC\n",
        "    # Not all models support predict_proba, so we check safely\n",
        "    proba = None\n",
        "    if hasattr(model, \"predict_proba\"):\n",
        "        try:\n",
        "            proba = model.predict_proba(X_test)[:, 1]   # probability of the positive class (>50K)\n",
        "        except Exception:\n",
        "            proba = None\n",
        "\n",
        "    # Print a detailed classification report (precision/recall/F1 per class)\n",
        "    print(f\"\\n{name}\")\n",
        "    print(classification_report(y_test, pred, target_names=[\"<=50K\", \">50K\"], zero_division=0))\n",
        "\n",
        "    # Confusion matrix helps explain model errors clearly in viva\n",
        "    cm = confusion_matrix(y_test, pred)\n",
        "    plt.figure(figsize=(4.7, 4))\n",
        "    sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\", cbar=False)\n",
        "    plt.title(f\"Confusion Matrix: {name}\")\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    # Store core metrics for later comparison across models\n",
        "    metrics = {\n",
        "        \"Model\": name,\n",
        "        \"Accuracy\": accuracy_score(y_test, pred),\n",
        "        \"Precision\": precision_score(y_test, pred, zero_division=0),\n",
        "        \"Recall\": recall_score(y_test, pred, zero_division=0),\n",
        "        \"F1\": f1_score(y_test, pred, zero_division=0),\n",
        "    }\n",
        "\n",
        "    # ROC-AUC and PR-AUC require probabilities, so compute only if available\n",
        "    # PR-AUC is especially informative when the positive class is smaller\n",
        "    if proba is not None:\n",
        "        metrics[\"ROC-AUC\"] = roc_auc_score(y_test, proba)\n",
        "        metrics[\"PR-AUC\"] = average_precision_score(y_test, proba)\n",
        "\n",
        "        # Optional: show ROC and Precision–Recall curves if requested\n",
        "        if show_curves:\n",
        "            RocCurveDisplay.from_predictions(y_test, proba)\n",
        "            plt.title(f\"ROC Curve: {name}\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "            PrecisionRecallDisplay.from_predictions(y_test, proba)\n",
        "            plt.title(f\"Precision–Recall Curve: {name}\")\n",
        "            plt.tight_layout()\n",
        "            plt.show()\n",
        "\n",
        "    return metrics\n",
        "\n",
        "# Logistic Regression baseline\n",
        "# Good interpretable linear baseline; saga solver works well with larger feature spaces (after one-hot encoding)\n",
        "logreg = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"clf\", LogisticRegression(max_iter=3000, solver=\"saga\", n_jobs=-1, random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "# Random Forest baseline\n",
        "# Strong non-linear model that can capture feature interactions without heavy assumptions\n",
        "rf = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"clf\", RandomForestClassifier(n_estimators=300, n_jobs=-1, random_state=RANDOM_STATE))\n",
        "])\n",
        "\n",
        "# Neural network baseline (MLP)\n",
        "# Early stopping helps avoid overfitting by stopping training when validation performance stops improving\n",
        "mlp = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"clf\", MLPClassifier(\n",
        "        hidden_layer_sizes=(64, 32),\n",
        "        activation=\"relu\",\n",
        "        solver=\"adam\",\n",
        "        early_stopping=True,\n",
        "        alpha=1e-4,                # small L2 regularization to reduce overfitting\n",
        "        learning_rate_init=1e-3,\n",
        "        max_iter=60 if FAST_MODE else 150,   # fewer iterations in FAST_MODE to save time\n",
        "        random_state=RANDOM_STATE\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Run evaluation for each baseline model and collect results in a list\n",
        "baseline = []\n",
        "baseline.append(evaluate(logreg, \"Logistic Regression (Baseline)\"))\n",
        "baseline.append(evaluate(rf, \"Random Forest (Baseline)\"))\n",
        "baseline.append(evaluate(mlp, \"Neural Network MLP (Baseline)\"))\n",
        "\n",
        "# Convert results into a DataFrame for easy comparison\n",
        "# Sorting by F1 is useful when class imbalance is present\n",
        "baseline_df = pd.DataFrame(baseline).sort_values(\"F1\", ascending=False)\n",
        "display(baseline_df.round(4))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WUkI-gh5htor"
      },
      "outputs": [],
      "source": [
        "# Use stratified k-fold cross-validation so each fold preserves class distribution\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=RANDOM_STATE)\n",
        "\n",
        "# Logistic Regression hyperparameter tuning\n",
        "# Regularization strength, penalty type, and class weighting are tuned\n",
        "# F1-score is used since the dataset is not perfectly balanced\n",
        "gs_lr = GridSearchCV(\n",
        "    logreg,\n",
        "    {\n",
        "        \"clf__penalty\": [\"l2\", \"l1\"],          # controls type of regularization\n",
        "        \"clf__C\": [0.05, 0.1, 0.5, 1, 5],      # inverse of regularization strength\n",
        "        \"clf__class_weight\": [None, \"balanced\"],  # helps handle class imbalance\n",
        "        \"clf__solver\": [\"saga\"],               # supports both L1 and L2 penalties\n",
        "    },\n",
        "    scoring=\"f1\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Run grid search on training data\n",
        "gs_lr.fit(X_train, y_train)\n",
        "\n",
        "# Random Forest hyperparameter tuning\n",
        "# Parameters mainly control model complexity and overfitting\n",
        "gs_rf = GridSearchCV(\n",
        "    rf,\n",
        "    {\n",
        "        \"clf__n_estimators\": [200, 400],       # number of trees in the forest\n",
        "        \"clf__max_depth\": [None, 10, 20],      # limits tree depth to reduce overfitting\n",
        "        \"clf__min_samples_split\": [2, 5, 10],  # minimum samples needed to split a node\n",
        "        \"clf__min_samples_leaf\": [1, 2, 4],    # minimum samples in a leaf node\n",
        "        \"clf__class_weight\": [None, \"balanced\"]  # balances classes during training\n",
        "    },\n",
        "    scoring=\"f1\",\n",
        "    cv=cv,\n",
        "    n_jobs=-1,\n",
        "    verbose=0\n",
        ")\n",
        "\n",
        "# Run grid search on training data\n",
        "gs_rf.fit(X_train, y_train)\n",
        "\n",
        "# Display best hyperparameters and corresponding cross-validation F1 scores\n",
        "print(\"Best LR params:\", gs_lr.best_params_)\n",
        "print(\"Best LR CV F1:\", round(gs_lr.best_score_, 4))\n",
        "print(\"Best RF params:\", gs_rf.best_params_)\n",
        "print(\"Best RF CV F1:\", round(gs_rf.best_score_, 4))\n",
        "\n",
        "# Extract the best-performing models from grid search\n",
        "best_lr = gs_lr.best_estimator_\n",
        "best_rf = gs_rf.best_estimator_\n",
        "\n",
        "# Evaluate tuned models on the test set\n",
        "# ROC and Precision–Recall curves are shown for deeper performance analysis\n",
        "tuned = []\n",
        "tuned.append(evaluate(best_lr, \"Logistic Regression (Tuned)\", show_curves=True))\n",
        "tuned.append(evaluate(best_rf, \"Random Forest (Tuned)\", show_curves=True))\n",
        "\n",
        "# Compare tuned models using F1-score\n",
        "display(pd.DataFrame(tuned).round(4).sort_values(\"F1\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCmvj_AKi2Z7"
      },
      "source": [
        "Interpretation:\n",
        "Hyperparameter tuning tries multiple settings using cross-validation and selects the best based on F1-score. This improves reliability because results are averaged across multiple folds rather than depending on a single train/test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fn5a_TVli3Dk"
      },
      "outputs": [],
      "source": [
        "# Final Models (optimal hyperparameters + selected features) and comparative analysis\n",
        "\n",
        "def count_selected_features(pipeline):\n",
        "    \"\"\"Count how many features remain after the feature-selection step.\"\"\"\n",
        "    # Fit the pipeline so the selector learns which features to keep\n",
        "    pipeline.fit(X_train, y_train)\n",
        "\n",
        "    # Try to access the feature selection step by name\n",
        "    selector = pipeline.named_steps.get(\"select\", None)\n",
        "    if selector is None:\n",
        "        return None\n",
        "\n",
        "    # get_support() returns a boolean mask for selected features\n",
        "    if hasattr(selector, \"get_support\"):\n",
        "        return int(selector.get_support().sum())\n",
        "\n",
        "    return None\n",
        "\n",
        "# Logistic Regression with embedded feature selection using L1 regularization\n",
        "# L1 pushes unimportant feature weights to zero, so SelectFromModel can drop them\n",
        "lr_fs = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"select\", SelectFromModel(\n",
        "        LogisticRegression(\n",
        "            penalty=\"l1\",\n",
        "            solver=\"saga\",\n",
        "            C=1.0,\n",
        "            max_iter=5000,\n",
        "            random_state=RANDOM_STATE\n",
        "        )\n",
        "    )),\n",
        "    # Use the tuned Logistic Regression classifier from grid search\n",
        "    (\"clf\", best_lr.named_steps[\"clf\"])\n",
        "])\n",
        "\n",
        "# Random Forest with embedded feature selection using tree-based feature importance\n",
        "rf_fs = Pipeline([\n",
        "    (\"prep\", preprocess),\n",
        "    (\"select\", SelectFromModel(\n",
        "        RandomForestClassifier(\n",
        "            n_estimators=400,\n",
        "            random_state=RANDOM_STATE,\n",
        "            n_jobs=-1\n",
        "        )\n",
        "    )),\n",
        "    # Use the tuned Random Forest classifier from grid search\n",
        "    (\"clf\", best_rf.named_steps[\"clf\"])\n",
        "])\n",
        "\n",
        "final_results = []\n",
        "\n",
        "# Train, cross-validate, and test both final models\n",
        "for model, name in [(lr_fs, \"Final Logistic Regression\"), (rf_fs, \"Final Random Forest\")]:\n",
        "    # Cross-validation score on training data gives a more stable estimate than one split\n",
        "    cv_f1 = cross_val_score(\n",
        "        model,\n",
        "        X_train,\n",
        "        y_train,\n",
        "        cv=cv,\n",
        "        scoring=\"f1\",\n",
        "        n_jobs=-1\n",
        "    ).mean()\n",
        "\n",
        "    # Count selected features for the final comparison table\n",
        "    n_feat = count_selected_features(model)\n",
        "\n",
        "    # Evaluate on test set (and show ROC + PR curves)\n",
        "    metrics = evaluate(model, name, show_curves=True)\n",
        "\n",
        "    # Add feature count and CV score into the metrics dictionary\n",
        "    metrics[\"Selected Features\"] = n_feat\n",
        "    metrics[\"CV F1\"] = cv_f1\n",
        "\n",
        "    final_results.append(metrics)\n",
        "\n",
        "# Build final comparison table\n",
        "final_table = pd.DataFrame(final_results)\n",
        "\n",
        "# Arrange columns to match the assignment template as closely as possible\n",
        "cols = [\"Model\", \"Selected Features\", \"CV F1\", \"Accuracy\", \"Precision\", \"Recall\", \"F1\", \"ROC-AUC\", \"PR-AUC\"]\n",
        "final_table = final_table[[c for c in cols if c in final_table.columns]].round(4)\n",
        "\n",
        "# Show the best model first based on F1-score\n",
        "display(final_table.sort_values(\"F1\", ascending=False))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_0OoxXwdcHU"
      },
      "source": [
        "## 7. Feature Selection (embedded methods)\n",
        "\n",
        "We apply **embedded feature selection** because it selects features *during model training* and naturally matches our models:\n",
        "\n",
        "1. **Logistic Regression (L1 penalty)**: L1 regularization drives unimportant coefficients to **exactly zero**, so we can keep only the non-zero (important) encoded features.\n",
        "2. **Random Forest feature importance**: Tree-based models compute impurity-based importances; `SelectFromModel` retains the most informative features based on importance thresholding.\n",
        "\n",
        "Why this is suitable here:\n",
        "- The dataset contains many categorical variables; one-hot encoding creates **many sparse features**. Feature selection reduces dimensionality, can improve generalization, and makes the model easier to explain.\n",
        "\n",
        "---\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}